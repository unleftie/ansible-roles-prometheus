# https://samber.github.io/awesome-prometheus-alerts/rules#prometheus-self-monitoring
groups:
  - name: prometheus
    rules:
      - alert: PrometheusExporterDown
        expr: "up{job=~'node_exporter'} == 0"
        for: 1m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus exporter seems down"
          description: "The node exporter is not responding\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusConfigurationReloadFailure
        expr: "prometheus_config_last_reload_successful != 1"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
          description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTooManyRestarts
        expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 4'
        for: 0m
        labels:
          severity: warn
          env: prod
        annotations:
          summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
          description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTargetsEmpty
        expr: "prometheus_sd_discovered_targets == 0"
        for: 5m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus targets empty"

      - alert: PrometheusJobMissing
        expr: "absent(up{job='prometheus'})"
        for: 5m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus job missing (instance {{ $labels.instance }})"
          description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerJobMissing
        expr: "absent(up{job='alertmanager'})"
        for: 5m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus AlertManager job missing (instance {{ $labels.instance }})"
          description: "A Prometheus AlertManager job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusNotConnectedToAlertmanager
        expr: "prometheus_notifications_alertmanagers_discovered < 1"
        for: 5m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTemplateTextExpansionFailures
        expr: "increase(prometheus_template_text_expansion_failures_total[3m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus template text expansion failures (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusNotificationsBacklog
        expr: "min_over_time(prometheus_notifications_queue_length[10m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus notifications backlog (instance {{ $labels.instance }})"
          description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerNotificationFailing
        expr: "rate(alertmanager_notifications_failed_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
          description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCheckpointCreationFailures
        expr: "increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCheckpointDeletionFailures
        expr: "increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCompactionsFailed
        expr: "increase(prometheus_tsdb_compactions_failed_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbHeadTruncationsFailed
        expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbReloadFailures
        expr: "increase(prometheus_tsdb_reloads_failures_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbWalCorruptions
        expr: "increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbWalTruncationsFailed
        expr: "increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0"
        for: 0m
        labels:
          severity: error
          env: prod
        annotations:
          summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
